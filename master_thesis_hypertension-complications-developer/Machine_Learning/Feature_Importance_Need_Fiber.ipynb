{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import ML_Pipeline_Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiber\n",
    "from fiber.cohort import Cohort\n",
    "from fiber.condition import Patient, MRNs\n",
    "from fiber.condition import Diagnosis\n",
    "from fiber.condition import Measurement, Encounter, Drug, TobaccoUse, AlcoholUse, MetaData\n",
    "from fiber.storage import yaml as fiberyaml\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import reduce \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from category_encoders import OneHotEncoder, TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import inspect\n",
    "import json\n",
    "import sys\n",
    "from pydoc import locate\n",
    "\n",
    "import requests\n",
    "\n",
    "#get all the model config and utility functions from the other file\n",
    "from ML_Pipeline_Overall import *\n",
    "\n",
    "persist = False\n",
    "window = 0\n",
    "input_filename = 'All3_ML_pipeline_final.pkl'\n",
    "output_path = '../plots/'\n",
    "output_filename = 'Plots_first_try.pkl'\n",
    "\n",
    "##################\n",
    "class FriendlyNamesConverter:\n",
    "    def rename_columns(self, df):\n",
    "        replacements = {}\n",
    "        for column in df.columns:\n",
    "            replacements[column] = self.get(column)\n",
    "        return replacements\n",
    "\n",
    "    def get(self, feature):\n",
    "        # does not support time window information inside feature name yet\n",
    "        if feature.startswith(('age', 'gender', 'religion', 'race')):\n",
    "            return feature.replace('_', ' ').replace('.', '|')\n",
    "\n",
    "        split_name = feature.split('__')\n",
    "        if len(split_name) > 1: \n",
    "            if split_name[1] in [\n",
    "                i[0]\n",
    "                for i in inspect.getmembers(\n",
    "                    sys.modules['fiber.condition'],\n",
    "                    inspect.isclass\n",
    "                )\n",
    "            ]:\n",
    "                aggregation = split_name[0]\n",
    "                split_name = split_name[1:]\n",
    "            else:\n",
    "                aggregation = None\n",
    "\n",
    "            if len(split_name) == 3:\n",
    "                class_name, context, code = split_name\n",
    "                condition_class = locate(f'fiber.condition.{class_name}')\n",
    "                description = self.get_description(condition_class, code, context)\n",
    "                if  \"Lipid panel\" in description:\n",
    "                    description = \"Lipid panel\"\n",
    "            else:\n",
    "                class_name, description = split_name\n",
    "\n",
    "            if aggregation is not None: \n",
    "                return f'{class_name} | {description.capitalize()} ({aggregation})'\n",
    "            else:\n",
    "                return f'{class_name} | {description.capitalize()}'\n",
    "        else:\n",
    "            return feature\n",
    "\n",
    "    def get_description(self, condition_class, code, context):\n",
    "        return condition_class(\n",
    "            code=code,\n",
    "            context=context\n",
    "        ).patients_per(\n",
    "            condition_class.description_column\n",
    "        )[\n",
    "            condition_class.description_column.name.lower()\n",
    "        ].iloc[0]\n",
    "\n",
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "\n",
    "def get_shap_importanceplot(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int, 'uint8'] and c not in ['Complication']]\n",
    "    train_df['Complication'] = pd.to_numeric(train_df['Complication'])\n",
    "    test_df['Complication'] = pd.to_numeric(test_df['Complication'])\n",
    "    print(\"length of column names are ::\" + str(len(categorical_cols) + len(numerical_cols)))\n",
    "\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols),])\n",
    "    \n",
    "    lgb_classifier = LGBMClassifier(**lgb_param_calibrated)\n",
    "    xgb_classifier = XGBClassifier(**xgb_param)\n",
    "    catboost_classifier = CatBoostClassifier(**catboost_param)\n",
    "\n",
    "    retro_train = train_df[train_df.columns.difference(['Complication'])]\n",
    "    retro_label = train_df['Complication']\n",
    "    pros_train = test_df[test_df.columns.difference(['Complication'])]\n",
    "    pros_label = test_df['Complication']\n",
    "\n",
    "    \n",
    "    preprocessed_data = column_transformer.fit_transform(retro_train, retro_label)\n",
    "    column_names = get_column_names_from_ColumnTransformer(column_transformer)\n",
    "\n",
    "    preprocessed_data = pd.DataFrame(preprocessed_data, columns = column_names)\n",
    "\n",
    "    #lgb_classifier.fit(preprocessed_data, train_df['Complication'])\n",
    "    #xgb_classifier.fit(preprocessed_data, train_df['Complication'])\n",
    "    \n",
    "    catboost_classifier.fit(preprocessed_data, train_df['Complication'])\n",
    "    \n",
    "    #preprocessed_test_data = column_transformer.transform(pros_train)\n",
    "    ##preprocessed_test_data = pd.DataFrame(preprocessed_test_data,column_names)\n",
    "    \n",
    "    shap_values = shap.TreeExplainer(catboost_classifier).shap_values(preprocessed_data)\n",
    "    f = plt.figure()\n",
    "    shap.summary_plot(shap_values, preprocessed_data, plot_type = 'dot')\n",
    "    f.savefig((f'shap_explained_catboost_{input_filename}.png'), bbox_inches='tight', dpi=600)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if (not os.path.isfile(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Machine_Learning/plots', output_filename))) or (persist == True):\n",
    "        print(\"renamed file not found, so creating one....\")\n",
    "        \n",
    "        import fiber\n",
    "        df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2011/', input_filename))\n",
    "        \n",
    "        rename_dict = FriendlyNamesConverter().rename_columns(df)\n",
    "        print(rename_dict)\n",
    "        print(df.shape)\n",
    "        \n",
    "        df = df.rename(columns=rename_dict, errors=\"raise\")\n",
    "        df = df.loc[:,~df.columns.duplicated()]\n",
    "        print(df.shape)\n",
    "        df.to_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Machine_Learning/plots', output_filename))\n",
    "    \n",
    "    else: \n",
    "        print(\"renamed file found, reading it from the drive....\")\n",
    "        df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Machine_Learning/plots', output_filename))\n",
    "\n",
    "    \n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "\n",
    "    print(df.shape)\n",
    "    print(\"final dataframe shape after dropping NAs\" + str(df.shape))\n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    \n",
    "    #df = df.reset_index('medical_record_number')\n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio) \n",
    "    \n",
    "    get_shap_importanceplot(train_df,test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
