{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import gc\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, plot_roc_curve, roc_curve, f1_score, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading needed DataFrames\n",
    "\n",
    "Dataset_A = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/Cerebro_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "Dataset_B = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/Renal_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "Dataset_C = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/Heart_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "\n",
    "# Choose Data Set\n",
    "df = Dataset_C\n",
    "\n",
    "# Set threshold for NaN -> at least 80 columns must be filled with non-naN values per row, otherwise drops row\n",
    "#df = df.dropna(axis =0, thresh=30)\n",
    "#df.loc[df['Complication'] == 0]\n",
    "\n",
    "# Set Labels\n",
    "cases = df.loc[df[\"Complication\"] == 1]\n",
    "control = df.loc[df[\"Complication\"] == 0]\n",
    "\n",
    "# Sample data\n",
    "cases = cases.sample(100)\n",
    "control = control.sample(100)\n",
    "df = pd.concat([control, cases])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation for other models than lgbm\n",
    "\n",
    "def impute_df_mean(df):\n",
    "    # Diagnosis - fill NaN with 0 \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"Diagnosis\")]]\n",
    "    df_NaN_0 = df_NaN.fillna(0)\n",
    "    # update back with df\n",
    "    df.update(df_NaN_0)\n",
    "    # Procedure - fill NaN with 0\n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"Procedure\")]]\n",
    "    df_NaN_0 = df_NaN.fillna(0)\n",
    "    # update back with df\n",
    "    df.update(df_NaN_0)\n",
    "    # Drug - fill NaN with 0 \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"Drug\")]]\n",
    "    df_NaN_0 = df_NaN.fillna(0)\n",
    "    # update back with  df\n",
    "    df.update(df_NaN_0)\n",
    "    # impute lab values and vital signs with mean of columns \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"VitalSign\")]]\n",
    "    df_NaN_0 = df.fillna(df.mean())\n",
    "    # update back with  df\n",
    "    df.update(df_NaN_0)\n",
    "    # impute lab values and vital signs with mean of columns \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"LabValue\")]]\n",
    "    df_NaN_0 = df.fillna(df.mean())\n",
    "    # update back with  df\n",
    "    df.update(df_NaN_0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename dataset and model\n",
    "To_train = df\n",
    "\n",
    "#impute\n",
    "To_train = impute_df_mean(To_train)\n",
    "\n",
    "model = 'lgbm'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Split Data #####\n",
    "target = np.array(To_train[\"Complication\"])\n",
    "train = To_train.drop(\"Complication\", axis= 1)\n",
    "feature_list = list(train.columns)\n",
    "features = np.array(train)\n",
    "\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(features, target, test_size = 0.25, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Normalisation to a range between 0 and 1\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_features = min_max_scaler.fit_transform(train_features)\n",
    "test_features = min_max_scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "\n",
    "#param = {'objective': 'binary', 'metric': ['auc', 'binary_logloss']}\n",
    "\n",
    "#grid = [{'num_leaves': [31, 127],\n",
    "        #'reg_alpha': [0.1, 0.5],\n",
    "        #'min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "        #'lambda_l1': [0, 1, 1.5],\n",
    "        #'lambda_l2': [0, 1]\n",
    "    #}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions #####\n",
    "\n",
    "def trainModel(classifier,train_features, train_targets, test_features):\n",
    "    crf = lgb_classifier \n",
    "    train_data = lgb.Dataset(train_features,label=train_targets, feature_name=feature_list)\n",
    "\n",
    "    test_pred = crf.predict(test_features)\n",
    "    train_pred = crf.predict(train_features)\n",
    "    \n",
    "    if (model == 'lgbm'):\n",
    "        for i in range(0,train_pred.shape[0]):\n",
    "            if train_pred[i] >= .5:       # setting threshold to .5\n",
    "                train_pred[i] = 1\n",
    "            else:  \n",
    "                train_pred[i] = 0\n",
    "        for i in range(0,test_pred.shape[0]):\n",
    "            if test_pred[i] >= .5:       # setting threshold to .5\n",
    "                test_pred[i] = 1\n",
    "            else:   \n",
    "                test_pred[i] = 0\n",
    "    \n",
    "    return crf, test_pred, train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(crf, train_targets, train_pred, test_targets, test_pred, test_features):\n",
    "        \n",
    "    roc_train = roc_auc_score(train_targets, train_pred)\n",
    "    roc_test = roc_auc_score(test_targets, test_pred)\n",
    "    \n",
    "    cm =confusion_matrix(test_targets, test_pred)\n",
    "\n",
    "    print(\" ROC of train:\", roc_train, \"\\n\", \"ROC of test:\", roc_test, \"\\n\", \"Confusion matrix:\", \"\\n\", cm)\n",
    "\n",
    "    # Sensitivity/Recall = TP / (TP + FN)\n",
    "    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    print('Sensitivity: ', sensitivity )\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    specificity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "    print('Specificity: ', specificity)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    precision = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "    print('Precision: ', precision)\n",
    "    \n",
    "    # F1 Score\n",
    "    print(\"F1 score: \" , f1_score(test_targets, test_pred))\n",
    "    \n",
    "    # APS\n",
    "    print(\"Average Precision Score: \", average_precision_score(test_targets, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_pipeline(train_features, test_features, train_targets, test_targets, \n",
    "                       lgb_classifier, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=lgb_classifier,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(train_features, train_targets)\n",
    "    \n",
    "    if do_probabilities:\n",
    "      pred = fitted_model.predict_proba(test_features)\n",
    "    else:\n",
    "      pred = fitted_model.predict(test_features)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8, 1],\n",
    "    'max_depth': [-1,7,15,20,25],\n",
    "    'num_leaves': [50, 100, 200],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'min_split_gain': [0.3, 0.4],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1],\n",
    "    'subsample_freq': [0, 20]\n",
    "}\n",
    "\n",
    "lgb_classifier, pred = algorithm_pipeline(train_features, test_features, train_targets, test_targets, lgb_classifier, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "\n",
    "classifier = lgb_classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf, test_pred, train_pred = trainModel(classifier, train_features, train_targets, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(crf, train_targets, train_pred, test_targets, test_pred, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
