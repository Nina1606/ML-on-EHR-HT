{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, plot_roc_curve, roc_curve, f1_score, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin\n",
    "import shap\n",
    "\n",
    "# Loading needed DataFrames\n",
    "\n",
    "Dataset_A = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/Cerebro_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "Dataset_B = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/Renal_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "Dataset_C = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/Heart_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "\n",
    "Dataset_D = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/CR_and_Heart_Cohort_Unsupervised_Features_All_Clean.pkl\")\n",
    "Dataset_E = pd.read_pickle(\"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/All3_Cohort_Unsupervised_Features_All_Clean.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = pd.read_pickle('/home/kiwitn01/master_thesis_hypertension-complications/ML_dataset_180.pkl')\n",
    "Test = Test.reindex(sorted(Test.columns), axis=1)\n",
    "Test.loc[Test['train_test'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Dataset_E for ML_pipelina\n",
    "#Dataset_E = Dataset_E.reset_index('medical_record_number')\n",
    "Dataset_E['train_test']= 'train'\n",
    "\n",
    "#samling 50% to be 'test'\n",
    "fifty_percent_sample = Dataset_E.sample(frac = 0.5, replace = False, axis = 0) \n",
    "fifty_percent_sample['train_test']= 'test'\n",
    "#updating dataset_E\n",
    "\n",
    "Dataset_E.update(fifty_percent_sample)\n",
    "\n",
    "#update Complications to ints\n",
    "Dataset_E['Complication'] = Dataset_E['Complication'].astype(int)\n",
    "\n",
    "#Dataset_E.loc[(Dataset_E['train_test'] == 'test') & (Dataset_E['Complication'] == 1)]\n",
    "\n",
    "#Dataset_E.to_pickle('All3Cohorts_Unsupervised_ML_pipeline.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Data Set\n",
    "\n",
    "#adjust cases and controls\n",
    "df = Dataset_E\n",
    "\n",
    "# Set threshold for NaN -> at least 80 columns must be filled with non-naN values per row, otherwise drops row\n",
    "df = df.dropna(axis =0, thresh=80)\n",
    "#df.loc[df['Complication'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Labels\n",
    "cases = df.loc[df[\"Complication\"] == 1]\n",
    "control = df.loc[df[\"Complication\"] == 0]\n",
    "\n",
    "# Sample data\n",
    "#cases = cases.sample(300)\n",
    "control = control.sample(10000)\n",
    "df = pd.concat([control, cases])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation for other models than lgbm\n",
    "\n",
    "def impute_df_mean(df):\n",
    "    # Diagnosis - fill NaN with 0 \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"Diagnosis\")]]\n",
    "    df_NaN_0 = df_NaN.fillna(0)\n",
    "    # update back with df\n",
    "    df.update(df_NaN_0)\n",
    "    # Procedure - fill NaN with 0\n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"Procedure\")]]\n",
    "    df_NaN_0 = df_NaN.fillna(0)\n",
    "    # update back with df\n",
    "    df.update(df_NaN_0)\n",
    "    # Drug - fill NaN with 0 \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"Drug\")]]\n",
    "    df_NaN_0 = df_NaN.fillna(0)\n",
    "    # update back with  df\n",
    "    df.update(df_NaN_0)\n",
    "    # impute lab values and vital signs with mean of columns \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"VitalSign\")]]\n",
    "    df_NaN_0 = df.fillna(df.mean())\n",
    "    # update back with  df\n",
    "    df.update(df_NaN_0)\n",
    "    # impute lab values and vital signs with mean of columns \n",
    "    df_NaN = df[df.columns[pd.Series(df.columns).str.contains(\"LabValue\")]]\n",
    "    df_NaN_0 = df.fillna(df.mean())\n",
    "    # update back with  df\n",
    "    df.update(df_NaN_0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Parameter Settings #####\n",
    "To_train = df\n",
    "\n",
    "# Model Selection\n",
    "model = 'lgbm'\n",
    "\n",
    "#impute\n",
    "#To_train = impute_df_mean(To_train)\n",
    " \n",
    "if (model == 'lgbm'):\n",
    "    classifier = lgb.LGBMClassifier()        \n",
    "    param = {'objective': 'binary', # for binary classification\n",
    "        'boost_from_average': False,\n",
    "        'is_unbalance': True,\n",
    "        'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "        'learning_rate': 0.0001,\n",
    "        'num_leaves': 250,\n",
    "        'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "        'max_depth': 45, # <0 means no limit\n",
    "        'max_bin': 512, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "        'lambda_l1': 2, # L1 regularization\n",
    "        'lambda_l2': 0, # L2 regularization\n",
    "        'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "        'subsample': 1, # subsample ratio of the training instance\n",
    "        'colsample_bytree': 0.05, # subsample ratio of columns when constructing the tree\n",
    "        'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "        'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "        'min_child_samples': 5, # minimum number of data needed in a leaf\n",
    "        'feature_fraction': 0.5,\n",
    "        'metric' : 'auc',\n",
    "        'reg_alpha': 0.5,\n",
    "        'reg_lambda': 0.5,\n",
    "        'bagging_fraction':0.8,\n",
    "        'bagging_freq':10          \n",
    "        }\n",
    "\n",
    "else:\n",
    "    print('Not supported model!')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions #####\n",
    "\n",
    "def trainModel(classifier,train_features, train_targets, test_features):\n",
    "\n",
    "    if (model == 'lgbm'):\n",
    "        train_data = lgb.Dataset(train_features,label=train_targets, feature_name=feature_list)\n",
    "        crf = lgb.train(param,train_data,valid_sets=[lgb_train, lgb_test], num_boost_round=800)\n",
    "        \n",
    "    else:\n",
    "        print(\"Not supported model!\")\n",
    "    \n",
    "    test_pred = crf.predict(test_features)\n",
    "    train_pred = crf.predict(train_features)\n",
    "    \n",
    "    if (model == 'lgbm'):\n",
    "        for i in range(0,train_pred.shape[0]):\n",
    "            if train_pred[i] >= .5:       # setting threshold to .5\n",
    "                train_pred[i] = 1\n",
    "            else:  \n",
    "                train_pred[i] = 0\n",
    "        for i in range(0,test_pred.shape[0]):\n",
    "            if test_pred[i] >= .5:       # setting threshold to .5\n",
    "                test_pred[i] = 1\n",
    "            else:   \n",
    "                test_pred[i] = 0\n",
    "    \n",
    "    return crf, test_pred, train_pred\n",
    "\n",
    "def evaluateModel(crf, train_targets, train_pred, test_targets, test_pred, test_features):\n",
    "        \n",
    "    roc_train = roc_auc_score(train_targets, train_pred)\n",
    "    roc_test = roc_auc_score(test_targets, test_pred)\n",
    "    \n",
    "    cm =confusion_matrix(test_targets, test_pred)\n",
    "\n",
    "    print(\" ROC of train:\", roc_train, \"\\n\", \"ROC of test:\", roc_test, \"\\n\", \"Confusion matrix:\", \"\\n\", cm)\n",
    "\n",
    "    # Sensitivity/Recall = TP / (TP + FN)\n",
    "    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    print('Sensitivity: ', sensitivity )\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    specificity = cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "    print('Specificity: ', specificity)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    precision = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "    print('Precision: ', precision)\n",
    "    \n",
    "    # F1 Score\n",
    "    print(\"F1 score: \" , f1_score(test_targets, test_pred))\n",
    "    \n",
    "    # APS\n",
    "    print(\"Average Precision Score: \", average_precision_score(test_targets, test_pred))\n",
    "    \n",
    "\n",
    "# Required for lgbm, as it does not accept special json characters\n",
    "if (model == 'lgbm'):\n",
    "    To_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in To_train.columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Split Data #####\n",
    "\n",
    "target = np.array(To_train[\"Complication\"])\n",
    "train = To_train.drop(\"Complication\", axis= 1)\n",
    "feature_list = list(train.columns)\n",
    "features = np.array(train)\n",
    "\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(features, target, test_size = 0.25, random_state = 42)\n",
    "\n",
    "lgb_train = lgb.Dataset(train_features,train_targets)\n",
    "lgb_test = lgb.Dataset(test_features,test_targets)\n",
    "\n",
    "\n",
    "# Feature Normalisation to a range between 0 and 1\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_features = min_max_scaler.fit_transform(train_features)\n",
    "test_features = min_max_scaler.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf, test_pred, train_pred = trainModel(classifier, train_features, train_targets, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel(crf, train_targets, train_pred, test_targets, test_pred, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
