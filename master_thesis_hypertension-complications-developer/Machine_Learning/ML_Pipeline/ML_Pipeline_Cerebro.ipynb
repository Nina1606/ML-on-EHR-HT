{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from sklearn.model_selection import cross_val_predict, cross_validate, KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score, accuracy_score, brier_score_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import OneHotEncoder, TargetEncoder\n",
    "from category_encoders import *\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from fancyimpute import IterativeSVD, BiScaler\n",
    "from ppca import PPCA\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.metrics import AUC\n",
    "import category_encoders as ce\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "input_filename = 'Cerebro_ML_pipeline_final.pkl'\n",
    "\n",
    "\n",
    "output_filename = 'Only_Extra_BP_Cerebro_final.csv'\n",
    "NA_removal_threshold = 80 ##atleast this many columns should have a non null value: 80 for unsupervised\n",
    "prediction_window = 0\n",
    "test_df_control_ratio = 2\n",
    "train_df_control_ratio = 2\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "########################################\n",
    "\n",
    "xgb_param = {'silent': 0, 'n_jobs': -1, 'max_depth': 15, 'reg_alpha': 1, 'reg_lambda': 1, 'random_state': 42, 'learning_rate': 0.05, 'max_bin': 32,\n",
    "    'colsample_bytree': 0.20, 'min_child_weight': 0.5, 'min_split_loss': 0.5, 'subsample': 0.5 }\n",
    "\n",
    "\n",
    "\n",
    "catboost_param = {'max_depth': 5, 'random_state': 42, 'learning_rate': 0.01, 'max_bin': 32,\n",
    "                  'objective': 'Logloss','eval_metric': 'AUC', 'iterations': 1200, 'verbose': 50, \n",
    "                  'l2_leaf_reg': 2, 'boosting_type': 'Plain', 'boost_from_average': False,\n",
    "                  'grow_policy': 'Lossguide',\n",
    "                  'min_data_in_leaf': 15,'max_leaves': 45,\n",
    "                  'custom_metric': ['Logloss', 'AUC'] \n",
    "                 }\n",
    "\n",
    "\n",
    "lgb_param_calibrated = {'objective': 'binary', # for binary classification\n",
    "        'boost_from_average': False,\n",
    "        'is_unbalance': True,\n",
    "        'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 250,\n",
    "        'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "        'max_depth': 15, # <0 means no limit\n",
    "        'max_bin': 32, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "        'lambda_l1': 2, # L1 regularization\n",
    "        'lambda_l2': 2, # L2 regularization\n",
    "        'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "        'subsample': 1, # subsample ratio of the training instance\n",
    "        'colsample_bytree': 0.2, # subsample ratio of columns when constructing the tree\n",
    "        'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "        'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "        'min_child_samples': 5, # minimum number of data needed in a leaf\n",
    "        'feature_fraction': 0.5,\n",
    "        'metric' : 'auc',\n",
    "        'reg_alpha': 1,\n",
    "        'reg_lambda': 1,\n",
    "        'bagging_fraction':0.8,\n",
    "        'bagging_freq':10          \n",
    "        }\n",
    "\n",
    "lgb_param = {'num_leaves':40, 'objective':'binary','max_depth':7,'learning_rate':0.01,'max_bin':128, 'metric': ['auc', 'binary_logloss'],\n",
    " 'n_jobs':-1, 'reg_alpha': 0.5, 'reg_lambda': 1, 'random_state': 42,'feature_fraction': 0.25}\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def twoLayerFeedForward():\n",
    "\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(250, input_dim=403, activation='relu')) #input_dim needs to be number of features(columns), after NaN dropped\n",
    "    clf.add(Dropout(0.3))\n",
    "    clf.add(Dense(100, activation='relu'))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    clf.compile(loss='binary_crossentropy', optimizer = Adam(lr=1e-3), metrics=[AUC()])\n",
    "\n",
    "    return clf\n",
    "#########################################################\n",
    "\n",
    "def train_test_split(df, test_df_control, train_df_control):\n",
    "    \n",
    "    train_df = df[df.train_test == 'train']\n",
    "    test_df = df[df.train_test == 'test']\n",
    "\n",
    "    if test_df_control_ratio is not None:\n",
    "        test_df_ht = test_df[test_df.Complication == '1']\n",
    "        test_df_not_ht = test_df[test_df.Complication == '0']\n",
    "        test_df_not_ht = test_df_not_ht.sample(int(test_df_control_ratio * test_df_ht.shape[0]), random_state=42)\n",
    "        test_df = pd.concat([test_df_ht,test_df_not_ht])\n",
    "        \n",
    "    if train_df_control_ratio is not None:\n",
    "        train_df_ht = train_df[train_df.Complication == '1']\n",
    "        train_df_not_ht = train_df[train_df.Complication == '0']\n",
    "        train_df_not_ht = train_df_not_ht.sample(int(train_df_control_ratio * train_df_ht.shape[0]), random_state=42)\n",
    "        train_df = pd.concat([train_df_ht,train_df_not_ht])\n",
    "    \n",
    "    mrn_train = set(train_df['medical_record_number'])\n",
    "    mrn_test = set(test_df['medical_record_number'])\n",
    "    train_test_common = mrn_train.intersection(mrn_test)\n",
    "    train_df = train_df.drop(columns=['train_test', 'medical_record_number'], errors = 'ignore')\n",
    "    test_df = test_df.drop(columns=['train_test', 'medical_record_number'], errors = 'ignore')\n",
    "    return train_df, test_df\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def get_calibration_plots(y_test, y_score, name):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    prob_pos = y_score\n",
    "    prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (name, ))\n",
    "\n",
    "    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "                histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Calibration_{0}.pdf'.format(name))\n",
    "    \n",
    "#########################################################\n",
    "    \n",
    "def evaluate_models(dict_models, train_df, test_df):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for key, value in dict_models.items():\n",
    "        print(\"running model: \"+ str(key))\n",
    "        classifier = value\n",
    "        dict_results = {}\n",
    "\n",
    "        dict_results['model'] = key\n",
    "        dict_results['prediction_window'] = prediction_window\n",
    "        dict_results['test_df_control_ratio'] = test_df_control_ratio\n",
    "        dict_results['train_df_control_ratio'] = train_df_control_ratio\n",
    "        dict_results['NA_removal_threshold'] = NA_removal_threshold\n",
    "\n",
    "        train_df['Complication'] = pd.to_numeric(train_df['Complication'])\n",
    "        test_df['Complication'] = pd.to_numeric(test_df['Complication'])\n",
    "\n",
    "        retro_auc = []\n",
    "        retro_auprc = []\n",
    "        retro_recall = []\n",
    "        retro_precision = []\n",
    "        retro_f1 = []\n",
    "        retro_accuracy = []\n",
    "        retro_brier = []\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=2,random_state = 42, shuffle = True)\n",
    "\n",
    "        retro_train = train_df[train_df.columns.difference(['Complication'])]\n",
    "        retro_label = train_df['Complication']\n",
    "\n",
    "        print('Retro_Train Shape: ' + str(retro_train.shape))\n",
    "\n",
    "        roc_best = 0\n",
    "        best_classifier = None\n",
    "\n",
    "        for train_index, test_index in kf.split(retro_train,retro_label):\n",
    "        \n",
    "            classifier.fit(retro_train.iloc[train_index], retro_label.iloc[train_index])\n",
    "            y_test_pred = np.where(classifier.predict_proba(retro_train.iloc[test_index])[:, 1] > THRESHOLD, 1, 0)\n",
    "            y_score = classifier.predict_proba(retro_train.iloc[test_index])[:, 1]\n",
    "            \n",
    "            #get_calibration_plots(retro_label.iloc[test_index], y_score,(str(key)+'_train'))\n",
    "            roc_auc = roc_auc_score(retro_label.iloc[test_index], y_score, average='micro')\n",
    "            if (roc_auc > roc_best):\n",
    "                best_classifier = classifier\n",
    "            retro_auc.append(roc_auc_score(retro_label.iloc[test_index], y_score, average='micro'))\n",
    "            retro_auprc.append(average_precision_score(retro_label.iloc[test_index], y_score, average='micro', pos_label = 1))\n",
    "            retro_recall.append(recall_score(retro_label.iloc[test_index], y_test_pred))\n",
    "            retro_precision.append(precision_score(retro_label.iloc[test_index], y_test_pred))\n",
    "            retro_f1.append(f1_score(retro_label.iloc[test_index], y_test_pred, average='micro'))\n",
    "            retro_accuracy.append(accuracy_score(retro_label.iloc[test_index], y_test_pred))\n",
    "            retro_brier.append(brier_score_loss(retro_label.iloc[test_index], y_score, pos_label=1))\n",
    "\n",
    "        print(\"train cross val auc \" + str(np.mean(retro_auc)))\n",
    "        print(\"train cross val auprc \" + str(np.mean(retro_auprc)))\n",
    "        dict_results['retro_auc'] = np.mean(retro_auc)\n",
    "        dict_results['retro_auprc'] = np.mean(retro_auprc)\n",
    "        dict_results['retro_recall'] = np.mean(retro_recall)\n",
    "        dict_results['retro_precision'] = np.mean(retro_precision)\n",
    "        dict_results['retro_f1'] = np.mean(retro_f1)\n",
    "        dict_results['retro_accuracy'] = np.mean(retro_accuracy)\n",
    "        dict_results['retro_brier'] = np.mean(retro_brier)\n",
    "\n",
    "        \n",
    "        classifier = best_classifier\n",
    "        \n",
    "        y_test_pred = np.where(classifier.predict_proba(test_df[test_df.columns.difference(['Complication'])])[:, 1] > THRESHOLD, 1, 0)\n",
    "        y_score = classifier.predict_proba(test_df[test_df.columns.difference(['Complication'])])[:, 1]\n",
    "        \n",
    "\n",
    "        #get_calibration_plots(test_df['HT'], y_score,(str(key)+'_test'))\n",
    "        print(\"test auc \" + str(roc_auc_score(test_df['Complication'], y_score, average='micro')))\n",
    "        print(\"test auprc \" + str(average_precision_score(test_df['Complication'], y_score, average='micro', pos_label = 1)))\n",
    "        print(\"test F1 score  \" + str(f1_score(test_df['Complication'], y_test_pred, average='micro')))\n",
    "        dict_results['prospective_auc'] = roc_auc_score(test_df['Complication'], y_score, average='micro')\n",
    "        dict_results['prospective_auprc'] = average_precision_score(test_df['Complication'], y_score, average='micro', pos_label = 1)\n",
    "        dict_results['prospective_recall'] = recall_score(test_df['Complication'], y_test_pred)\n",
    "        dict_results['prospective_precision'] = precision_score(test_df['Complication'], y_test_pred)       \n",
    "        dict_results['prospective_F1'] = f1_score(test_df['Complication'], y_test_pred, average='micro')\n",
    "        dict_results['prospective_accuracy'] = accuracy_score(test_df['Complication'], y_test_pred)\n",
    "        dict_results['prospective_brier'] = brier_score_loss(test_df['Complication'], y_score, pos_label=1)\n",
    "        dict_results['Cohort'] = input_filename\n",
    "\n",
    "        \n",
    "        results.append(dict_results)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "   \n",
    "    classifier_lgb =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        #('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),]),\n",
    "        ##RFE(estimator=lgbm, n_features_to_select=50, step=10),\n",
    "        #('lgbm',  LGBMClassifier(**lgb_param_calibrated))\n",
    "        ('lgbm-calibrated', CalibratedClassifierCV(base_estimator=LGBMClassifier(**lgb_param_calibrated), cv=10, method='isotonic'))\n",
    "        ])\n",
    "\n",
    "\n",
    "    \n",
    "    dict_models = { 'lgbm-isotonic': classifier_lgb,\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/Only_extra_BP/', input_filename))\n",
    "    #df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    #df = df.drop(['marital_status_code'], axis = 1)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "    \n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "\n",
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "   \n",
    "    classifier_rf =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        ('simpleimputer' , SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "        ('random-forest', RandomForestClassifier(n_estimators=100))\n",
    "        ])\n",
    "\n",
    "\n",
    "    \n",
    "    dict_models = { 'random_forest': classifier_rf,\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/Only_extra_BP/', input_filename))\n",
    "    #df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    #df = df.drop(['marital_status_code'], axis = 1)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio,train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "      \n",
    "    classifier_lgb_simpleimpute =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        ('simpleimputer' , SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "        #('softimpute', IterativeSVD(max_iters = 500)),\n",
    "        #('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),]),\n",
    "        ##RFE(estimator=lgbm, n_features_to_select=50, step=10),\n",
    "        ('lgbm-calibrated', CalibratedClassifierCV(base_estimator=LGBMClassifier(**lgb_param_calibrated), cv=10, method='isotonic'))])\n",
    "\n",
    "\n",
    "    \n",
    "    dict_models = { \n",
    "                    'lgb-isotonic-simpleimpute_mean': classifier_lgb_simpleimpute,\n",
    "\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/Only_extra_BP/', input_filename))\n",
    "    #df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    #df = df.drop(['marital_status_code'], axis = 1)\n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio,train_df_control_ratio)\n",
    "    \n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "\n",
    "    \n",
    "    classifier_xgb =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        #('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),]),\n",
    "        #RFE(estimator=lgbm, n_features_to_select=50, step=10),\n",
    "       #('xgb',  XGBClassifier(**xgb_param))\n",
    "       ('xgb-calibrated', CalibratedClassifierCV(base_estimator=XGBClassifier(**xgb_param), cv=10, method='isotonic'))\n",
    "       ])\n",
    "\n",
    "    \n",
    "    dict_models = { \n",
    "                    'xgb-isotonic':  classifier_xgb,\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data/', input_filename))\n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    df = df.drop(['marital_status_code'], axis = 1)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "    \n",
    "    classifier_xgb_simpleimpute =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        ('simpleimputer' , SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "        #('softimpute', IterativeSVD(max_iters = 500)),\n",
    "        #('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),]),\n",
    "        ##RFE(estimator=lgbm, n_features_to_select=50, step=10),\n",
    "        ('xgb-calibrated', CalibratedClassifierCV(base_estimator=XGBClassifier(**xgb_param), cv=10, method='isotonic'))])\n",
    "\n",
    "        \n",
    "    dict_models = { \n",
    "                    'xgb-isotonic-simpleimpute_mean': classifier_xgb_simpleimpute,\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data/', input_filename))\n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    df = df.drop(['marital_status_code'], axis = 1)\n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "   \n",
    "    \n",
    "    classifier_lr_iter =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        ('iterativeimpute' , IterativeImputer(random_state=42, max_iter = 50, n_nearest_features = 5)),\n",
    "        #('softimpute', IterativeSVD(max_iters = 500)),\n",
    "        #('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),]),\n",
    "        ##RFE(estimator=lgbm, n_features_to_select=50, step=10),\n",
    "       ('lr',  LogisticRegression(penalty = 'l2', solver = 'saga', n_jobs = -1, random_state = 42, C = 1.2,  max_iter = 2000, l1_ratio  = 0.5))])\n",
    "\n",
    "\n",
    "    \n",
    "    dict_models = {'lr_iterativeimpute': classifier_lr_iter}\n",
    "\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data/', input_filename))\n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "   \n",
    "    classifier_lr_simpleimpute =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(drop_invariant = True, handle_missing = 'return_nan', min_samples_leaf = 10), categorical_cols),])),\n",
    "        ('simpleimputer' , SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "        #('softimpute', IterativeSVD(max_iters = 500)),\n",
    "        #('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),]),\n",
    "        ##RFE(estimator=lgbm, n_features_to_select=50, step=10),\n",
    "       ('lr',  LogisticRegression(penalty = 'l2', solver = 'saga', n_jobs = -1, random_state = 42, C = 1.2,  max_iter = 2000, l1_ratio  = 0.5))])\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    dict_models = {\n",
    "                    'lr_simpleimpute_mean': classifier_lr_simpleimpute,\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data/', input_filename))\n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "   \n",
    "   \n",
    "    classifier_catboost =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', CatBoostEncoder(drop_invariant = True, handle_missing = 'return_nan', sigma = 0.5, a = 0.7), categorical_cols),])),\n",
    "       ('catboost-calibrated', CalibratedClassifierCV(base_estimator=CatBoostClassifier(**catboost_param), cv=10, method='isotonic'))\n",
    "       ])\n",
    "\n",
    "\n",
    "    \n",
    "    dict_models = {\n",
    "                    'catboost-isotonic' : classifier_catboost,\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data/', input_filename))\n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_params(train_df, test_df):\n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols))) \n",
    "    \n",
    "    classifier_catboost_simpleimpute =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', CatBoostEncoder(drop_invariant = True, handle_missing = 'return_nan', sigma = 0.5, a = 0.7), categorical_cols),])),\n",
    "        ('simpleimputer' , SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "        ('catboost-calibrated', CalibratedClassifierCV(base_estimator=CatBoostClassifier(**catboost_param), cv=10, method='isotonic'))\n",
    "        ])\n",
    "\n",
    "    \n",
    "    dict_models = { 'catboost-isotonic-simpleimpute_mean': classifier_catboost_simpleimpute}\n",
    "\n",
    "    \n",
    "    \n",
    "    results = evaluate_models(dict_models, train_df,test_df)\n",
    "    return results\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def write_file(df_results, filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"output file doesn't exist, creating a new one...\")\n",
    "        df_results.to_csv(filepath, header=True, index = False)\n",
    "    else: \n",
    "        print(\"output file exists, appending to the existing file...\")\n",
    "        df_results.to_csv(filepath, mode='a', header=False, index = False)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data/', input_filename))\n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    \n",
    "    print(\"final dataframe shape after dropping NAs of threshold\" + str(df.shape))\n",
    "    \n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    #get_ppca(train_df, test_df)\n",
    "    results = model_params(train_df, test_df)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(df_results.head())\n",
    "\n",
    "write_file(df_results, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
