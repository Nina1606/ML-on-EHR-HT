{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import shap\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from sklearn.model_selection import cross_val_predict, cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score, accuracy_score, brier_score_loss, precision_recall_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import OneHotEncoder, TargetEncoder\n",
    "from category_encoders import *\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "\n",
    "import sklearn.impute\n",
    "from sklearn import impute\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import json\n",
    "from pydoc import locate\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##set random seeds\n",
    "np.random.seed(7)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'Renal_ML_pipeline_final.pkl'\n",
    "output_filename = 'results_with_renal_data_0_2_P.csv'\n",
    "NA_removal_threshold = 80 ##atleast this many columns should have a non null value: 80 for unsupervised\n",
    "prediction_window = 0\n",
    "test_df_control_ratio = 2\n",
    "train_df_control_ratio = 2\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "xgb_param = {'silent': 0, 'n_jobs': -1, 'max_depth': 15, 'reg_alpha': 1, 'reg_lambda': 1, 'random_state': 42, 'learning_rate': 0.05, 'max_bin': 32,\n",
    "    'colsample_bytree': 0.20, 'min_child_weight': 0.5, 'min_split_loss': 0.5, 'subsample': 0.5 }\n",
    "\n",
    "catboost_param = {'max_depth': 5, 'random_state': 42, 'learning_rate': 0.01, 'max_bin': 32,\n",
    "                  'objective': 'Logloss','eval_metric': 'AUC', 'iterations': 1200, 'verbose': 50, \n",
    "                  'l2_leaf_reg': 2, 'boosting_type': 'Plain', 'boost_from_average': False,\n",
    "                  'grow_policy': 'Lossguide',\n",
    "                  'min_data_in_leaf': 15,'max_leaves': 45,\n",
    "                  'custom_metric': ['Logloss', 'AUC'] \n",
    "                 }\n",
    "\n",
    "lgb_param_calibrated = {'objective': 'binary', # for binary classification\n",
    "        'boost_from_average': False,\n",
    "        'is_unbalance': True,\n",
    "        'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 250,\n",
    "        'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "        'max_depth': 15, # <0 means no limit\n",
    "        'max_bin': 32, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "        'lambda_l1': 2, # L1 regularization\n",
    "        'lambda_l2': 2, # L2 regularization\n",
    "        'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "        'subsample': 1, # subsample ratio of the training instance\n",
    "        'colsample_bytree': 0.2, # subsample ratio of columns when constructing the tree\n",
    "        'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "        'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "        'min_child_samples': 5, # minimum number of data needed in a leaf\n",
    "        'feature_fraction': 0.5,\n",
    "        'metric' : 'auc',\n",
    "        'reg_alpha': 1,\n",
    "        'reg_lambda': 1,\n",
    "        'bagging_fraction':0.8,\n",
    "        'bagging_freq':10          \n",
    "        }\n",
    "\n",
    "lgb_param = {'num_leaves':40, 'objective':'binary','max_depth':7,'learning_rate':0.01,'max_bin':128, 'metric': ['auc', 'binary_logloss'],\n",
    " 'n_jobs':-1, 'reg_alpha': 0.5, 'reg_lambda': 1, 'random_state': 42,'feature_fraction': 0.25}\n",
    "\n",
    "\n",
    "def train_test_split(df, test_df_control, train_df_control):\n",
    "    \n",
    "    train_df = df[df.train_test == 'train']\n",
    "    test_df = df[df.train_test == 'test']\n",
    "\n",
    "    if test_df_control_ratio is not None:\n",
    "        test_df_ht = test_df[test_df.Complication == '1']\n",
    "        test_df_not_ht = test_df[test_df.Complication == '0']\n",
    "        test_df_not_ht = test_df_not_ht.sample(int(test_df_control_ratio * test_df_ht.shape[0]), random_state=42)\n",
    "        test_df = pd.concat([test_df_ht,test_df_not_ht])\n",
    "        \n",
    "    if train_df_control_ratio is not None:\n",
    "        train_df_ht = train_df[train_df.Complication == '1']\n",
    "        train_df_not_ht = train_df[train_df.Complication == '0']\n",
    "        train_df_not_ht = train_df_not_ht.sample(int(train_df_control_ratio * train_df_ht.shape[0]), random_state=42)\n",
    "        train_df = pd.concat([train_df_ht,train_df_not_ht])\n",
    "    \n",
    "    mrn_train = set(train_df['medical_record_number'])\n",
    "    mrn_test = set(test_df['medical_record_number'])\n",
    "    train_test_common = mrn_train.intersection(mrn_test)\n",
    "    train_df = train_df.drop(columns=['train_test', 'medical_record_number'], errors = 'ignore')\n",
    "    test_df = test_df.drop(columns=['train_test', 'medical_record_number'], errors = 'ignore')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "class FriendlyNamesConverter:\n",
    "    def rename_columns(self, df):\n",
    "        replacements = {}\n",
    "        for column in df.columns:\n",
    "            replacements[column] = self.get(column)\n",
    "        return replacements\n",
    "\n",
    "    def get(self, feature):\n",
    "        # does not support time window information inside feature name yet\n",
    "        if feature.startswith(('age', 'gender', 'religion', 'race')):\n",
    "            return feature.replace('_', ' ').replace('.', '|')\n",
    "\n",
    "        split_name = feature.split('__')\n",
    "        if len(split_name) > 1: \n",
    "            if split_name[1] in [\n",
    "                i[0]\n",
    "                for i in inspect.getmembers(\n",
    "                    sys.modules['fiber.condition'],\n",
    "                    inspect.isclass\n",
    "                )\n",
    "            ]:\n",
    "                aggregation = split_name[0]\n",
    "                split_name = split_name[1:]\n",
    "            else:\n",
    "                aggregation = None\n",
    "\n",
    "            if len(split_name) == 3:\n",
    "                class_name, context, code = split_name\n",
    "                condition_class = locate(f'fiber.condition.{class_name}')\n",
    "                description = self.get_description(condition_class, code, context)\n",
    "                if  \"Lipid panel\" in description:\n",
    "                    description = \"Lipid panel\"\n",
    "            else:\n",
    "                class_name, description = split_name\n",
    "\n",
    "            if aggregation is not None: \n",
    "                return f'{class_name} | {description.capitalize()} ({aggregation})'\n",
    "            else:\n",
    "                return f'{class_name} | {description.capitalize()}'\n",
    "        else:\n",
    "            return feature\n",
    "\n",
    "    def get_description(self, condition_class, code, context):\n",
    "        return condition_class(\n",
    "            code=code,\n",
    "            context=context\n",
    "        ).patients_per(\n",
    "            condition_class.description_column\n",
    "        )[\n",
    "            condition_class.description_column.name.lower()\n",
    "        ].iloc[0]\n",
    "\n",
    "\n",
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name\n",
    "\n",
    "\n",
    "def get_shap_importanceplot(train_df, test_df):\n",
    "    \n",
    "\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int, 'uint8'] and c not in ['Complication']]\n",
    "    train_df['Complication'] = pd.to_numeric(train_df['Complication'])\n",
    "    test_df['Complication'] = pd.to_numeric(test_df['Complication'])\n",
    "    \n",
    "    print(\"length of column names are ::\" + str(len(categorical_cols) + len(numerical_cols)))\n",
    "\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', TargetEncoder(), categorical_cols),])\n",
    "        \n",
    "    \n",
    "    xgb_classifier = XGBClassifier(**xgb_param)\n",
    "\n",
    "    retro_train = train_df[train_df.columns.difference(['Complication'])]\n",
    "    retro_label = train_df['Complication']\n",
    "    pros_train = test_df[test_df.columns.difference(['Complication'])]\n",
    "    pros_label = test_df['Complication']\n",
    "\n",
    "    \n",
    "    preprocessed_data = column_transformer.fit_transform(retro_train, retro_label)\n",
    "    column_names = get_column_names_from_ColumnTransformer(column_transformer)\n",
    "\n",
    "    preprocessed_data = pd.DataFrame(preprocessed_data, columns = column_names)\n",
    "\n",
    "    xgb_classifier.fit(preprocessed_data, train_df['Complication'])\n",
    "    \n",
    "    shap.initjs()\n",
    "    \n",
    "    shap_values = shap.TreeExplainer(xgb_classifier).shap_values(preprocessed_data)\n",
    "    f = plt.figure()\n",
    "    shap.summary_plot(shap_values, preprocessed_data, plot_type = 'dot')\n",
    "    \n",
    "    f.savefig((f'shap_final_xgb_extraBP_TargetEnc_{input_filename}.png'), bbox_inches='tight', dpi=600)\n",
    "    \n",
    "    #save SHAP values as dataframe\n",
    "    shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "    importance_df = pd.DataFrame([preprocessed_data.columns.tolist(), shap_sum.tolist()]).T\n",
    "    importance_df.columns = ['column_name', 'shap_importance']\n",
    "    importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "    print (importance_df.head())\n",
    "    importance_df.to_csv('/home/kiwitn01/master_thesis_hypertension-complications/Machine_Learning/ML_Pipeline/SHAP_agg_data/Final_SHAP/Renal_final_extraBP_TargetEnc.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012', input_filename))\n",
    "    df =  pd.read_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/with_extra_BP_and_demographic_data', input_filename))\n",
    "           \n",
    "    print(df.shape)\n",
    "    df.to_pickle(os.path.join('/home/kiwitn01/master_thesis_hypertension-complications/Machine_Learning/plots', output_filename))\n",
    "    \n",
    "    df = df.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    df = df.drop(['marital_status_code'], axis = 1)\n",
    "    \n",
    "    #rename_dict = FriendlyNamesConverter().rename_columns(df)\n",
    "    #df = df.rename(columns=rename_dict)\n",
    "    #df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(\"final dataframe shape after dropping NAs\" + str(df.shape))\n",
    "    print(pd.crosstab(df.train_test, df.Complication))\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_df_control_ratio, train_df_control_ratio)\n",
    "    \n",
    "    get_shap_importanceplot(train_df,test_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
