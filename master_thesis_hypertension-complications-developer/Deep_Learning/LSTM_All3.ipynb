{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, f1_score, roc_auc_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "#%load_ext tensorboard\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import concatenate,Dense\n",
    "from keras.models import Model,load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from pathlib import Path\n",
    "from directories import *\n",
    "import numpy as np\n",
    "from parameters_config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(BASE_DIRECTORY.absolute())\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import OneHotEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "##set random seeds\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(seed=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_removal_threshold = 80  ##atleast this many columns should have a non null value\n",
    "test_df_control_ratio = None\n",
    "\n",
    "BASE_PATH='/home/kiwitn01/master_thesis_hypertension-complications/Time_Series/'\n",
    "\n",
    "# SET COHORT PATH\n",
    "\n",
    "#with extra BP data\n",
    "DATA_FILE_PATH = \"/home/kiwitn01/master_thesis_hypertension-complications/Time_Series/New/Dynamic_features/only_extra_BP/\"\n",
    "\n",
    "#without extra BP data\n",
    "#DATA_FILE_PATH = \"/home/kiwitn01/master_thesis_hypertension-complications/Time_Series/New/Dynamic_features/without_extra_BP_and_without_nan_numeric/\"\n",
    "\n",
    "DATA_UNSUPERVISED_PATH = \"/home/kiwitn01/master_thesis_hypertension-complications/Time_Series/Static_features/\"\n",
    "\n",
    "MRN_PATH = \"/home/kiwitn01/master_thesis_hypertension-complications/Case_Control_Cohort_Creation/For_ML_Pipeline/Split_2012/All3_ML_pipeline_final.pkl\"\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "Config.VOCAB_SIZE= 524 # number of unique concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence_data(df_mrn):\n",
    "    with open(DATA_FILE_PATH+\"data_all3_2012.txt\", \"rb\") as fp:\n",
    "        X = pickle.load(fp)\n",
    "\n",
    "    df = df_mrn.merge(pd.DataFrame(X, columns=['medical_record_number','sequence']),how='right',on='medical_record_number')\n",
    "\n",
    "    return df[['medical_record_number', 'sequence', 'Complication']]\n",
    "\n",
    "\n",
    "def load_unsupervised_data(path):\n",
    "    df = pd.read_pickle(path+'All3_2012_Drug_Diag_Static.pkl')\n",
    "    #for only extra BP data!\n",
    "    df = df[['Complication', 'train_test', 'marital_status_code', 'medical_record_number']]\n",
    "    df = df.drop(['train_test'], axis=1)\n",
    "    df = df.drop(['marital_status_code'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def train_test_split_custom(df, test_df_control):\n",
    "    train_df = df[df.train_test == 'train']\n",
    "    test_df = df[df.train_test == 'test']\n",
    "\n",
    "    if test_df_control_ratio is not None:\n",
    "        test_df_ht = test_df[test_df.Complication == '1']\n",
    "        test_df_not_ht = test_df[test_df.Complication == '0']\n",
    "        test_df_not_ht = test_df_not_ht.sample(test_df_control_ratio * test_df_ht.shape[0], random_state=42)\n",
    "        test_df = pd.concat([test_df_ht,test_df_not_ht])\n",
    "    \n",
    "    mrn_train = set(train_df['medical_record_number'])\n",
    "    mrn_test = set(test_df['medical_record_number'])\n",
    "    train_test_common = mrn_train.intersection(mrn_test)\n",
    "    train_df = train_df.drop(columns=['train_test'])\n",
    "    test_df = test_df.drop(columns=['train_test'])\n",
    "    return train_df, test_df\n",
    "\n",
    "def filter_by_mrn(df,mrn_list):\n",
    "    return df[df.medical_record_number.isin(mrn_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import category_encoders as ce\n",
    "from category_encoders import *\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "\n",
    "\n",
    "def impute_unsupervised_data(train_df, test_df):\n",
    "    categorical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.object] and c not in ['Complication']]\n",
    "    numerical_cols = [c for c in train_df.columns if train_df[c].dtype in [np.float, np.int] and c not in ['Complication']]\n",
    "    print(\"Number of categorical features \" + str(len(categorical_cols)) + \" and number of numerical features \"+ str(len(numerical_cols)))\n",
    "    \n",
    "    ct =  Pipeline([('ct',\n",
    "        ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        #('cat', HelmertEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),])),\n",
    "        #('cat', CatBoostEncoder(drop_invariant = True, handle_missing = 'return_nan', sigma = 0.5, a = 0.7), categorical_cols),])),\n",
    "        ('cat', OneHotEncoder(drop_invariant = True, handle_missing = 'return_nan'), categorical_cols),])),\n",
    "\n",
    "        #('iterativeimpute' , IterativeImputer(random_state=42, max_iter = 50, n_nearest_features = 5)),\n",
    "        ('simpleimputer' , SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "        #('KNN' , KNNImputer(n_neighbors =2)),\n",
    "        ])\n",
    "\n",
    "    fitted_train= ct.fit(train_df,pd.to_numeric(train_labels))\n",
    "    \n",
    "    train_df = fitted_train.transform(train_df)\n",
    "    test_df = fitted_train.transform(test_df)\n",
    "    \n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Masking\n",
    "\n",
    "def train_lstm_model(train_sequence_data, train_unsupervised_data, train_labels):\n",
    "\n",
    "    train_sequence_data, val_sequence_data, train_unsupervised_data, val_unsupervised_data, train_labels , val_labels = train_test_split(train_sequence_data, train_unsupervised_data, train_labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    print(train_sequence_data.shape)\n",
    "    print(val_sequence_data.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(val_labels.shape)\n",
    "\n",
    "    main_input = keras.Input(shape=(train_sequence_data.shape[1],), name='main_input') # dtype='int32'\n",
    "\n",
    "    # This embedding layer will encode the input sequence\n",
    "    # into a sequence of dense 512-dimensional vectors.\n",
    "    x = layers.Embedding(Config.VOCAB_SIZE, Config.EMBEDDING_DIM, input_length=Config.MAX_REVIEW_LENGTH, name='Embedding_1')(main_input)\n",
    "    \n",
    "    mask= layers.Masking(mask_value = -100, name = 'mask')(x)\n",
    "    \n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "    lstm_1 = layers.LSTM(100, name=\"lstm_1\", dropout=0.5, return_sequences=True)(mask)\n",
    "    lstm_2 = layers.LSTM(100, name='lstm_2', dropout=0.5, return_sequences=True)(lstm_1)\n",
    "    lstm_out = layers.LSTM(100, name='lstm_out', dropout=0.5)(lstm_2)\n",
    "    \n",
    "    aux_input=keras.Input(shape=(train_unsupervised_data.shape[1],),name='aux_input')\n",
    "    \n",
    "    # We concatenate the lstm output to auxillary input\n",
    "    x = concatenate([lstm_out, aux_input])\n",
    "    \n",
    "    dense_1 = Dense(100, activation='sigmoid', name='dense_1')(x)\n",
    "    \n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(dense_1)\n",
    "    \n",
    "    model = Model(inputs=[main_input, aux_input], outputs=[main_output])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=Config.METRICS)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        verbose=1,\n",
    "        patience=20,\n",
    "        mode='max',\n",
    "        restore_best_weights=True)\n",
    "    \n",
    "    try:\n",
    "        with tf.device('/device:GPU:2'):\n",
    "            history = model.fit({'main_input': train_sequence_data, 'aux_input': train_unsupervised_data},\n",
    "                {'main_output': train_labels},\n",
    "                epochs=200,\n",
    "                validation_data=([val_sequence_data,val_unsupervised_data],val_labels),\n",
    "                batch_size=None, verbose=1, callbacks=[early_stopping], shuffle = True) \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "    val_predictions = model.predict([val_sequence_data,val_unsupervised_data])\n",
    "    val_auprc = average_precision_score(val_labels, val_predictions, average='micro', pos_label = 1)\n",
    "    val_auc = roc_auc_score(val_labels, val_predictions, average='micro')\n",
    "    val_f1 = f1_score(val_labels, np.where(val_predictions > 0.5, 1, 0), average='micro')\n",
    "    print(\"val AUPRC is :: \" + str(val_auprc))\n",
    "    print(\"val AUC is :: \" + str(val_auc))\n",
    "    print(\"val F1 is ::\" + str(val_f1))\n",
    "\n",
    " \n",
    "    epch = early_stopping.stopped_epoch\n",
    "\n",
    "    validation_score = [history.history['val_loss'][epch],\n",
    "        history.history['val_tp'][epch],\n",
    "        history.history['val_fp'][epch],\n",
    "        history.history['val_tn'][epch],\n",
    "        history.history['val_fn'][epch],\n",
    "        history.history['val_accuracy'][epch],\n",
    "        history.history['val_precision'][epch],\n",
    "        history.history['val_recall'][epch],\n",
    "        history.history['val_auc'][epch],\n",
    "        history.history['val_f1_score'][epch],\n",
    "        history.history['val_average_precision'][epch]]\n",
    "    \n",
    "    print('Validation Score:',validation_score)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a CNN model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Conv1D, Flatten\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, TimeDistributed\n",
    "\n",
    "def train_lstm_model(train_sequence_data, train_unsupervised_data, train_labels):\n",
    "\n",
    "    train_sequence_data, val_sequence_data, train_unsupervised_data, val_unsupervised_data, train_labels , val_labels = train_test_split(train_sequence_data, train_unsupervised_data, train_labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    print(train_sequence_data.shape)\n",
    "    print(val_sequence_data.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(val_labels.shape)\n",
    "      \n",
    "\n",
    "    #first input layer of sequence, categorical data\n",
    "    main_input = keras.Input(shape=(train_sequence_data.shape[1],), name='main_input') # dtype='int32'\n",
    "\n",
    "    # This embedding layer will encode the input sequence into a sequence of dense 512-dimensional vectors.\n",
    "    x = layers.Embedding(Config.VOCAB_SIZE, Config.EMBEDDING_DIM, input_length=Config.MAX_REVIEW_LENGTH, name='Embedding_1')(main_input)\n",
    "    \n",
    "    #add convolutional layer\n",
    "    convolution_1 = tf.keras.layers.Conv1D(100, 2, activation=\"relu\")(x)\n",
    "    pooling1 = tf.keras.layers.MaxPooling1D(pool_size=2)(convolution_1)\n",
    "\n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "    lstm_out = layers.LSTM(100, name='lstm_3', dropout=0.5)(pooling1)\n",
    "    \n",
    "    \n",
    "    #add second input layer\n",
    "    aux_input=keras.Input(shape=(train_unsupervised_data.shape[1],),name='aux_input')\n",
    "    \n",
    "    \n",
    "    # We concatenate the lstm output to auxillary input\n",
    "    x = concatenate([lstm_out, aux_input])\n",
    "    \n",
    "    flatten = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    dense_1 = Dense(100, activation='sigmoid', name='dense_1')(flatten)\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(dense_1)\n",
    "    \n",
    "    model = Model(inputs=[main_input, aux_input], outputs=[main_output])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=Config.METRICS)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        verbose=1,\n",
    "        patience=20,\n",
    "        mode='max',\n",
    "        restore_best_weights=True)\n",
    "    \n",
    "    try:\n",
    "        with tf.device('/device:GPU:2'):\n",
    "            history = model.fit({'main_input': train_sequence_data, 'aux_input': train_unsupervised_data},\n",
    "                {'main_output': train_labels},\n",
    "                epochs=200,\n",
    "                validation_data=([val_sequence_data,val_unsupervised_data],val_labels),\n",
    "                batch_size=None, verbose=1, callbacks=[early_stopping], shuffle = True) \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "    val_predictions = model.predict([val_sequence_data,val_unsupervised_data])\n",
    "    val_auprc = average_precision_score(val_labels, val_predictions, average='micro', pos_label = 1)\n",
    "    val_auc = roc_auc_score(val_labels, val_predictions, average='micro')\n",
    "    val_f1 = f1_score(val_labels, np.where(val_predictions > 0.5, 1, 0), average='micro')\n",
    "    print(\"val AUPRC is :: \" + str(val_auprc))\n",
    "    print(\"val AUC is :: \" + str(val_auc))\n",
    "    print(\"val F1 is ::\" + str(val_f1))\n",
    "\n",
    " \n",
    "    epch = early_stopping.stopped_epoch\n",
    "\n",
    "    validation_score = [history.history['val_loss'][epch],\n",
    "        history.history['val_tp'][epch],\n",
    "        history.history['val_fp'][epch],\n",
    "        history.history['val_tn'][epch],\n",
    "        history.history['val_fn'][epch],\n",
    "        history.history['val_accuracy'][epch],\n",
    "        history.history['val_precision'][epch],\n",
    "        history.history['val_recall'][epch],\n",
    "        history.history['val_auc'][epch],\n",
    "        history.history['val_f1_score'][epch],\n",
    "        history.history['val_average_precision'][epch]]\n",
    "    \n",
    "    print('Validation Score:',validation_score)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old less layers\n",
    "\n",
    "def train_lstm_model(train_sequence_data, train_unsupervised_data, train_labels):\n",
    "\n",
    "    train_sequence_data, val_sequence_data, train_unsupervised_data, val_unsupervised_data, train_labels , val_labels = train_test_split(train_sequence_data, train_unsupervised_data, train_labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    print(train_sequence_data.shape)\n",
    "    print(val_sequence_data.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(val_labels.shape)\n",
    "\n",
    "    main_input = keras.Input(shape=(train_sequence_data.shape[1],), name='main_input') # dtype='int32'\n",
    "\n",
    "    # This embedding layer will encode the input sequence\n",
    "    # into a sequence of dense 512-dimensional vectors.\n",
    "    x = layers.Embedding(Config.VOCAB_SIZE, Config.EMBEDDING_DIM, input_length=Config.MAX_REVIEW_LENGTH, name='Embedding_1')(main_input)\n",
    "    \n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "    lstm_out = layers.LSTM(100, name='lstm_1', dropout=0.3)(x)\n",
    "    aux_input=keras.Input(shape=(train_unsupervised_data.shape[1],),name='aux_input')\n",
    "    \n",
    "    # We concatenate the lstm output to auxillary input\n",
    "    x = concatenate([lstm_out, aux_input])\n",
    "    \n",
    "    drop_out = tf.keras.layers.Dropout(0.3) (x)\n",
    "    dense_1 = Dense(100, activation='sigmoid',\n",
    "                    kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.02),\n",
    "                    activity_regularizer=regularizers.l2(0.02), name='dense_1')(drop_out)\n",
    "    drop_out2 = tf.keras.layers.Dropout(0.3) (dense_1)\n",
    "    \n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(drop_out2)\n",
    "    \n",
    "    model = Model(inputs=[main_input, aux_input], outputs=[main_output])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=Config.METRICS)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        verbose=1,\n",
    "        patience=20,\n",
    "        mode='max',\n",
    "        restore_best_weights=True)\n",
    "    \n",
    "    try:\n",
    "        with tf.device('/device:GPU:2'):\n",
    "            history = model.fit({'main_input': train_sequence_data, 'aux_input': train_unsupervised_data},\n",
    "                {'main_output': train_labels},\n",
    "                epochs=1,\n",
    "                validation_data=([val_sequence_data,val_unsupervised_data],val_labels),\n",
    "                batch_size=None, verbose=1, callbacks=[early_stopping], shuffle = True) \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "    val_predictions = model.predict([val_sequence_data,val_unsupervised_data])\n",
    "    val_auprc = average_precision_score(val_labels, val_predictions, average='micro', pos_label = 1)\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_labels, val_predictions, average='micro')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    val_f1 = f1_score(val_labels, np.where(val_predictions > 0.5, 1, 0), average='micro')\n",
    "    print(\"val AUPRC is :: \" + str(val_auprc))\n",
    "    print(\"val AUC is :: \" + str(val_auc))\n",
    "    print(\"val F1 is ::\" + str(val_f1))\n",
    "\n",
    " \n",
    "    epch = early_stopping.stopped_epoch\n",
    "\n",
    "    validation_score = [history.history['val_loss'][epch],\n",
    "        history.history['val_tp'][epch],\n",
    "        history.history['val_fp'][epch],\n",
    "        history.history['val_tn'][epch],\n",
    "        history.history['val_fn'][epch],\n",
    "        history.history['val_accuracy'][epch],\n",
    "        history.history['val_precision'][epch],\n",
    "        history.history['val_recall'][epch],\n",
    "        history.history['val_auc'][epch],\n",
    "        history.history['val_f1_score'][epch],\n",
    "        history.history['val_average_precision'][epch]]\n",
    "    \n",
    "    print('Validation Score:',validation_score)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm_model(model, test_sequence_data, test_labels):\n",
    "    results = model.evaluate([test_sequence_data], test_labels, batch_size=256, verbose=1)\n",
    "    print(model.metrics_names)\n",
    "    print(results)\n",
    "    test_predictions = model.predict([test_sequence_data])\n",
    "    test_auprc = average_precision_score(test_labels, test_predictions, average='micro', pos_label = 1)\n",
    "    test_auc = roc_auc_score(test_labels, test_predictions, average='micro')\n",
    "    test_f1 = f1_score(test_labels, np.where(test_predictions > 0.5, 1, 0), average='micro')\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(test_labels, test_predictions)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    ix = np.argmax(fscore)\n",
    "    print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f ' % (thresholds[ix], fscore[ix], precision[ix], recall[ix]))\n",
    "\n",
    "    print(\"test AUPRC is :: \" + str(test_auprc))\n",
    "    print(\"test AUC is :: \" + str(test_auc))\n",
    "    print(\"test F1 is ::\" + str(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ##load train test mrns\n",
    "    df_mrn = pd.read_pickle(MRN_PATH)\n",
    "\n",
    "    #load sequence data\n",
    "    sequence_data = load_sequence_data(df_mrn)\n",
    "    sequence_data = sequence_data.sort_values(by=['medical_record_number'])\n",
    "    \n",
    "    df_mrn = df_mrn.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "    print(\"final dataframe shape after dropping NAs\" + str(df_mrn.shape))\n",
    "    \n",
    "    MRN_train_df, MRN_test_df = train_test_split_custom(df_mrn, test_df_control_ratio)\n",
    "    train_mrn = list(MRN_train_df.medical_record_number)\n",
    "    test_mrn = list(MRN_test_df.medical_record_number)\n",
    "\n",
    "\n",
    "    ##filter by mrns\n",
    "    train_sequence_data = filter_by_mrn(sequence_data, train_mrn)\n",
    "    test_sequence_data = filter_by_mrn(sequence_data, test_mrn)\n",
    "\n",
    "    train_labels_seq = train_sequence_data.pop('Complication').astype('int')\n",
    "    test_labels_seq = test_sequence_data.pop('Complication').astype('int')\n",
    "\n",
    "    print(train_labels_seq.value_counts())\n",
    "    print(test_labels_seq.value_counts())\n",
    "\n",
    "    train_mrn_seq = train_sequence_data.pop('medical_record_number').astype('int')\n",
    "    test_mrn_seq = test_sequence_data.pop('medical_record_number').astype('int')\n",
    "    \n",
    "    #matching unsupervised & sequence data \n",
    "    train_mrn_seq_match = train_mrn_seq.map(str)\n",
    "    test_mrn_seq_match = test_mrn_seq.map(str)\n",
    "\n",
    "    train_mrn_seq_match = train_mrn_seq_match.tolist()\n",
    "    test_mrn_seq_match = test_mrn_seq_match.tolist()\n",
    "\n",
    "    ##load auxilliary data\n",
    "    unsupervised_data = load_unsupervised_data(DATA_UNSUPERVISED_PATH)\n",
    "    unsupervised_data = unsupervised_data.sort_values(by=['medical_record_number'])\n",
    "    \n",
    "    #\n",
    "    train_unsupervised_data = filter_by_mrn(unsupervised_data, train_mrn_seq_match)\n",
    "    test_unsupervised_data = filter_by_mrn(unsupervised_data, test_mrn_seq_match)\n",
    "\n",
    "\n",
    "    train_labels = train_unsupervised_data.pop('Complication').astype('int')\n",
    "    test_labels = test_unsupervised_data.pop('Complication').astype('int')\n",
    "\n",
    "    train_mrn_unsupervised = train_unsupervised_data.pop('medical_record_number').astype('int')\n",
    "    test_mrn_unsupervised = test_unsupervised_data.pop('medical_record_number').astype('int')\n",
    "    \n",
    "    ##impute unsupervised data\n",
    "    #train_unsupervised_data, test_unsupervised_data = impute_unsupervised_data(train_unsupervised_data,test_unsupervised_data)\n",
    "\n",
    "    ##pad the sequences\n",
    "    train_sequence_data = np.asarray(train_sequence_data['sequence'])\n",
    "    test_sequence_data = np.asarray(test_sequence_data['sequence'])\n",
    "    train_sequence_data = tf.keras.preprocessing.sequence.pad_sequences(train_sequence_data, maxlen=150, padding='pre', truncating='pre')\n",
    "    test_sequence_data = tf.keras.preprocessing.sequence.pad_sequences(test_sequence_data, maxlen=150, padding='pre', truncating='pre')\n",
    "    print('After padding the sequence with the longest length the shape is:',train_sequence_data.shape)\n",
    "    print(train_sequence_data.max())\n",
    "    print(test_sequence_data.max())\n",
    "\n",
    "\n",
    "    ##sanity checks\n",
    "    print(\"check if train labels are same\")\n",
    "    assert np.array_equal(train_labels_seq.to_numpy(), train_labels.to_numpy())\n",
    "\n",
    "    print(\"check if test labels are same\")\n",
    "    assert np.array_equal(test_labels_seq.to_numpy(), test_labels.to_numpy())\n",
    "\n",
    "    print(\"check if train mrns are same\")\n",
    "    assert np.array_equal(train_mrn_seq.to_numpy(), train_mrn_unsupervised.to_numpy())\n",
    "\n",
    "    print(\"check if test mrns are same\")\n",
    "    assert np.array_equal(test_mrn_seq.to_numpy(), test_mrn_unsupervised.to_numpy())\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "    model = train_lstm_model(train_sequence_data, train_unsupervised_data, train_labels_seq)\n",
    "    results = evaluate_lstm_model(model, test_sequence_data, test_unsupervised_data, test_labels_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FOR SHAP####\n",
    "\n",
    "##############################################\n",
    "\n",
    "##load train test mrns\n",
    "df_mrn = pd.read_pickle(MRN_PATH)\n",
    "\n",
    "##load sequence data\n",
    "sequence_data = load_sequence_data(df_mrn)\n",
    "sequence_data = sequence_data.sort_values(by=['medical_record_number'])\n",
    "\n",
    "df_mrn = df_mrn.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "print(\"final dataframe shape after dropping NAs\" + str(df_mrn.shape))\n",
    "\n",
    "MRN_train_df, MRN_test_df = train_test_split_custom(df_mrn, test_df_control_ratio)\n",
    "train_mrn = list(MRN_train_df.medical_record_number)\n",
    "test_mrn = list(MRN_test_df.medical_record_number)\n",
    "\n",
    "\n",
    "    ##filter by mrns only those taken relevant for the cohort\n",
    "    \n",
    "train_features_sequence = filter_by_mrn(sequence_data, train_mrn)\n",
    "test_features_sequence = filter_by_mrn(sequence_data, test_mrn)\n",
    "\n",
    "train_labels_seq = train_features_sequence.pop('Complication').astype('int')\n",
    "test_labels_seq = test_features_sequence.pop('Complication').astype('int')\n",
    "\n",
    "print(train_labels_seq.value_counts())\n",
    "print(test_labels_seq.value_counts())\n",
    "\n",
    "train_mrn_seq = train_features_sequence.pop('medical_record_number').astype('int')\n",
    "test_mrn_seq = test_features_sequence.pop('medical_record_number').astype('int')\n",
    "    \n",
    "#matching unsupervised & sequence data \n",
    "train_mrn_seq_match = train_mrn_seq.map(str)\n",
    "test_mrn_seq_match = test_mrn_seq.map(str)\n",
    "\n",
    "train_mrn_seq_match = train_mrn_seq_match.tolist()\n",
    "test_mrn_seq_match = test_mrn_seq_match.tolist()\n",
    "\n",
    "    ##load auxilliary data\n",
    "unsupervised_data = load_unsupervised_data(DATA_UNSUPERVISED_PATH)\n",
    "unsupervised_data = unsupervised_data.sort_values(by=['medical_record_number'])\n",
    "    \n",
    "    #\n",
    "train_features_unsupervised = filter_by_mrn(unsupervised_data, train_mrn_seq_match)\n",
    "test_features_unsupervised = filter_by_mrn(unsupervised_data, test_mrn_seq_match)\n",
    "\n",
    "\n",
    "train_labels = train_features_unsupervised.pop('Complication').astype('int')\n",
    "test_labels = test_features_unsupervised.pop('Complication').astype('int')\n",
    "\n",
    "train_mrn_unsupervised = train_features_unsupervised.pop('medical_record_number').astype('int')\n",
    "test_mrn_unsupervised = test_features_unsupervised.pop('medical_record_number').astype('int')\n",
    "\n",
    "    # drop 'Diagnosis__ICD-9__414.00', & 'Diagnosis__ICD-9__414.01', -> are being dropped by columntransformer & pipeline\n",
    "train_features_unsupervised = train_features_unsupervised.drop(['Diagnosis__ICD-9__414.00', 'Diagnosis__ICD-9__414.01'], axis = 1) \n",
    "test_features_unsupervised = test_features_unsupervised.drop(['Diagnosis__ICD-9__414.00', 'Diagnosis__ICD-9__414.01'], axis = 1)\n",
    "\n",
    "    ##impute unsupervised data\n",
    "train_features_unsupervised, test_features_unsupervised = impute_unsupervised_data(train_features_unsupervised,test_features_unsupervised)\n",
    "\n",
    "    ##pad the sequences\n",
    "train_features_sequence = np.asarray(train_features_sequence['sequence'])\n",
    "test_features_sequence = np.asarray(test_features_sequence['sequence'])\n",
    "train_features_sequence = tf.keras.preprocessing.sequence.pad_sequences(train_features_sequence, maxlen=Config.MAX_REVIEW_LENGTH, padding='pre', truncating='pre')\n",
    "test_features_sequence = tf.keras.preprocessing.sequence.pad_sequences(test_features_sequence, maxlen=Config.MAX_REVIEW_LENGTH, padding='pre', truncating='pre')\n",
    "print('After padding the sequence with the longest length the shape is:',train_features_sequence.shape)\n",
    "print(train_features_sequence.max())\n",
    "print(test_features_sequence.max())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "    ##sanity checks\n",
    "print(\"check if train labels are same\")\n",
    "assert np.array_equal(train_labels_seq.to_numpy(), train_labels.to_numpy())\n",
    "\n",
    "print(\"check if test labels are same\")\n",
    "assert np.array_equal(test_labels_seq.to_numpy(), test_labels.to_numpy())\n",
    "\n",
    "print(\"check if train mrns are same\")\n",
    "assert np.array_equal(train_mrn_seq.to_numpy(), train_mrn_unsupervised.to_numpy())\n",
    "\n",
    "print(\"check if test mrns are same\")\n",
    "assert np.array_equal(test_mrn_seq.to_numpy(), test_mrn_unsupervised.to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "train_features_sequence, val_features_sequence, train_features_unsupervised, val_features_unsupervised, train_labels , val_labels = train_test_split(train_features_sequence, train_features_unsupervised, train_labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(train_features_sequence.shape)\n",
    "print(val_features_sequence.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "\n",
    "main_input = keras.Input(shape=(train_features_sequence.shape[1],), name='main_input') # dtype='int32'\n",
    "\n",
    "    # This embedding layer will encode the input sequence\n",
    "    # into a sequence of dense 512-dimensional vectors.\n",
    "x = layers.Embedding(Config.VOCAB_SIZE, Config.EMBEDDING_DIM, input_length=Config.MAX_REVIEW_LENGTH, name='Embedding_1')(main_input)\n",
    "    \n",
    "    # A LSTM will transform the vector sequence into a single vector,\n",
    "    # containing information about the entire sequence\n",
    "lstm_1 = layers.LSTM(100, name=\"lstm_1\", dropout=0.3, return_sequences=True)(x)\n",
    "lstm_2 = layers.LSTM(100, name='lstm_2', dropout=0.3, return_sequences=True)(lstm_1)\n",
    "lstm_out = layers.LSTM(100, name='lstm_out', dropout=0.3)(lstm_2)\n",
    "    \n",
    "aux_input=keras.Input(shape=(train_features_unsupervised.shape[1],),name='aux_input')\n",
    "    \n",
    "# We concatenate the lstm output to auxillary input\n",
    "concat = concatenate([lstm_out, aux_input])\n",
    "\n",
    "dense_1 = Dense(100, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                bias_regularizer=regularizers.l2(0.02),\n",
    "                activity_regularizer=regularizers.l2(0.02), name='dense_1')(concat)\n",
    "\n",
    "#drop_out2 = tf.keras.layers.Dropout(0.3) (dense_1)\n",
    "\n",
    "#dense_2 = Dense(100, activation='sigmoid', name='dense_2')(drop_out2)\n",
    "    \n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(dense_1)\n",
    "    \n",
    "model = Model(inputs=[main_input, aux_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "    \n",
    "    # compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=Config.METRICS)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=20,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "    \n",
    "try:\n",
    "    with tf.device('/device:GPU:2'):\n",
    "        history = model.fit({'main_input': train_features_sequence, 'aux_input': train_features_unsupervised},\n",
    "            {'main_output': train_labels},\n",
    "            epochs=200,\n",
    "            validation_data=([val_features_sequence,val_features_unsupervised],val_labels),\n",
    "            batch_size=None, verbose=1, callbacks=[early_stopping], shuffle = True) \n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "        \n",
    "    \n",
    "val_predictions = model.predict([val_features_sequence,val_features_unsupervised])\n",
    "val_auprc = average_precision_score(val_labels, val_predictions, average='micro', pos_label = 1)\n",
    "val_auc = roc_auc_score(val_labels, val_predictions, average='micro')\n",
    "val_f1 = f1_score(val_labels, np.where(val_predictions > 0.5, 1, 0), average='micro')\n",
    "print(\"val AUPRC is :: \" + str(val_auprc))\n",
    "print(\"val AUC is :: \" + str(val_auc))\n",
    "print(\"val F1 is ::\" + str(val_f1))\n",
    "\n",
    " \n",
    "epch = early_stopping.stopped_epoch\n",
    "\n",
    "validation_score = [history.history['val_loss'][epch],\n",
    "    history.history['val_tp'][epch],\n",
    "    history.history['val_fp'][epch],\n",
    "    history.history['val_tn'][epch],\n",
    "    history.history['val_fn'][epch],\n",
    "    history.history['val_accuracy'][epch],\n",
    "    history.history['val_precision'][epch],\n",
    "    history.history['val_recall'][epch],\n",
    "    history.history['val_auc'][epch],\n",
    "    history.history['val_f1_score'][epch],\n",
    "    history.history['val_average_precision'][epch]]\n",
    "    \n",
    "print('Validation Score:',validation_score)\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "\n",
    "results = model.evaluate([test_features_sequence,test_features_unsupervised],\n",
    "                         test_labels, batch_size=256, verbose=1)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(results)\n",
    "test_predictions = model.predict([test_features_sequence,test_features_unsupervised])\n",
    "test_auprc = average_precision_score(test_labels, test_predictions, average='micro', pos_label = 1)\n",
    "test_auc = roc_auc_score(test_labels, test_predictions, average='micro')\n",
    "test_f1 = f1_score(test_labels, np.where(test_predictions > 0.5, 1, 0), average='micro')\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(test_labels, test_predictions)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f ' % (thresholds[ix], fscore[ix], precision[ix], recall[ix]))\n",
    "\n",
    "print(\"test AUPRC is :: \" + str(test_auprc))\n",
    "print(\"test AUC is :: \" + str(test_auc))\n",
    "print(\"test F1 is ::\" + str(test_f1))\n",
    "\n",
    "#model = train_lstm_model(train_sequence_data, train_unsupervised_data, train_labels_seq)\n",
    "#results = evaluate_lstm_model(model, test_sequence_data, test_unsupervised_data, test_labels_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features_sequence.shape)\n",
    "print(train_features_unsupervised.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SHAP #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install h5py==2.10.0\n",
    "#conda install -c conda-forge hdf5=2.10.0\n",
    "\n",
    "# Save the baseline model\n",
    "model.save('Baseline_model_all3_FINAL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('Baseline_model_all3_FINAL.h5', \n",
    "                custom_objects={\"f1_score\":Config.f1_score, \n",
    "              \"average_precision\":Config.average_precision}\n",
    ")\n",
    "\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP Feature Importance#\n",
    "\n",
    "import shap\n",
    "\n",
    "explainer = shap.DeepExplainer(loaded_model, [train_features_sequence[0:1000,:],np.asarray(train_features_unsupervised)[0:1000,:]])\n",
    "shap_values = explainer.shap_values([test_features_sequence[0:1000,:],np.asarray(test_features_unsupervised)[0:1000,:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values[0][0].shape) # For sequence data\n",
    "print(shap_values[0][1].shape) # For unsupervised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sequence data\n",
    "# Fetching unique concepts for sequence data\n",
    "unique_concepts = pd.read_csv('/home/kiwitn01/master_thesis_hypertension-complications/Time_Series/unique_concepts_30_days_no_numeric.csv')\n",
    "unique_concepts\n",
    "unique_concepts.columns =[ 'name', 'code'] \n",
    "concepts_dictionary = unique_concepts.to_dict('dict')\n",
    "concepts_dictionary = concepts_dictionary['name']\n",
    "concepts_dictionary = dict(zip(concepts_dictionary.values(), concepts_dictionary.keys()))\n",
    "concepts_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the number to text mapping for the selected sequence data with '0' as 'NONE'\n",
    "num2word = {}\n",
    "for w in concepts_dictionary.keys():\n",
    "    num2word[concepts_dictionary[w]] = w\n",
    "x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), test_features_sequence[i]))) for i in range(100)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_words.shape)\n",
    "print(shap_values[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting feature importance based on the shap values\n",
    "shap_values_dict = {}\n",
    "for i in range(0,x_test_words.shape[0]):\n",
    "    for j in range(0,x_test_words.shape[1]):\n",
    "        shap_list=[]\n",
    "        if x_test_words[i][j] not in ['NONE']:\n",
    "            if x_test_words[i][j] in shap_values_dict.keys():\n",
    "                shap_values_dict[x_test_words[i][j]].append(shap_values[0][0][i][j])\n",
    "            else:\n",
    "                shap_list.append(shap_values[0][0][i][j])\n",
    "                shap_values_dict[x_test_words[i][j]] = shap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging the feature importance\n",
    "from functools import reduce\n",
    "sequence_feature_importance_values = {}\n",
    "for key in shap_values_dict.keys():\n",
    "    shap_vals = shap_values_dict[key]\n",
    "    #abs_list = [abs(ele) for ele in shap_vals]\n",
    "    sum_list = [sum((ele) for ele in shap_vals)]\n",
    "    \n",
    "    average_val = reduce(lambda x, y:x+y, sum_list)/len(sum_list)\n",
    "    sequence_feature_importance_values[key] = average_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains the most important sequence features\n",
    "len(sequence_feature_importance_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "sorted_dict = dict(sorted(sequence_feature_importance_values.items(), key=lambda x: x[1], reverse=True))\n",
    "sorted_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature importance\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sequence_importance = pd.DataFrame.from_dict(sorted_dict, orient='index',columns=['rank'])\n",
    "sequence_importance.reset_index(inplace=True)\n",
    "sequence_importance.rename(columns={\"index\": \"name\"}, inplace=True)\n",
    "top_10 = sequence_importance.head(20)\n",
    "\n",
    "# print(top_10)\n",
    "ax = sns.barplot(x=\"rank\", y=\"name\", data=top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For aggregated data\n",
    "print(shap_values[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get columns back\n",
    "##load train test mrns\n",
    "df_mrn = pd.read_pickle(MRN_PATH)\n",
    "\n",
    "##load sequence data\n",
    "sequence_data = load_sequence_data(df_mrn)\n",
    "sequence_data = sequence_data.sort_values(by=['medical_record_number'])\n",
    "\n",
    "df_mrn = df_mrn.dropna(axis=0, thresh = NA_removal_threshold)\n",
    "print(\"final dataframe shape after dropping NAs\" + str(df_mrn.shape))\n",
    "\n",
    "MRN_train_df, MRN_test_df = train_test_split_custom(df_mrn, test_df_control_ratio)\n",
    "train_mrn = list(MRN_train_df.medical_record_number)\n",
    "test_mrn = list(MRN_test_df.medical_record_number)\n",
    "\n",
    "\n",
    "    ##filter by mrns only those taken relevant for the cohort\n",
    "    \n",
    "train_features_sequence = filter_by_mrn(sequence_data, train_mrn)\n",
    "test_features_sequence = filter_by_mrn(sequence_data, test_mrn)\n",
    "\n",
    "train_labels_seq = train_features_sequence.pop('Complication').astype('int')\n",
    "test_labels_seq = test_features_sequence.pop('Complication').astype('int')\n",
    "\n",
    "print(train_labels_seq.value_counts())\n",
    "print(test_labels_seq.value_counts())\n",
    "\n",
    "train_mrn_seq = train_features_sequence.pop('medical_record_number').astype('int')\n",
    "test_mrn_seq = test_features_sequence.pop('medical_record_number').astype('int')\n",
    "    \n",
    "#matching unsupervised & sequence data \n",
    "train_mrn_seq_match = train_mrn_seq.map(str)\n",
    "test_mrn_seq_match = test_mrn_seq.map(str)\n",
    "\n",
    "train_mrn_seq_match = train_mrn_seq_match.tolist()\n",
    "test_mrn_seq_match = test_mrn_seq_match.tolist()\n",
    "\n",
    "    ##load auxilliary data\n",
    "unsupervised_data = load_unsupervised_data(DATA_UNSUPERVISED_PATH)\n",
    "unsupervised_data = unsupervised_data.sort_values(by=['medical_record_number'])\n",
    "    \n",
    "    #\n",
    "train_features_unsupervised = filter_by_mrn(unsupervised_data, train_mrn_seq_match)\n",
    "test_features_unsupervised = filter_by_mrn(unsupervised_data, test_mrn_seq_match)\n",
    "\n",
    "\n",
    "train_labels = train_features_unsupervised.pop('Complication').astype('int')\n",
    "test_labels = test_features_unsupervised.pop('Complication').astype('int')\n",
    "\n",
    "train_mrn_unsupervised = train_features_unsupervised.pop('medical_record_number').astype('int')\n",
    "test_mrn_unsupervised = test_features_unsupervised.pop('medical_record_number').astype('int')\n",
    "\n",
    "    # drop 'Diagnosis__ICD-9__414.00', & 'Diagnosis__ICD-9__414.01', -> are being dropped by columntransformer & pipeline\n",
    "train_features_unsupervised = train_features_unsupervised.drop(['Diagnosis__ICD-9__414.00', 'Diagnosis__ICD-9__414.01'], axis = 1) \n",
    "test_features_unsupervised = test_features_unsupervised.drop(['Diagnosis__ICD-9__414.00', 'Diagnosis__ICD-9__414.01'], axis = 1)\n",
    "\n",
    "len(train_features_unsupervised.columns)\n",
    "\n",
    "shap_values_df = pd.DataFrame(shap_values[0][1], columns = train_features_unsupervised.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_vals = {}\n",
    "for i in range(0, len(shap_values_df.columns)):\n",
    "    column_name = shap_values_df.columns[i]\n",
    "    vals = shap_values_df[shap_values_df.columns[i]]\n",
    "    vals = vals.sum()\n",
    "    #vals = vals.abs()\n",
    "    mean = vals.mean()\n",
    "    shap_vals[column_name] = mean\n",
    "print(shap_vals)\n",
    "\n",
    "from collections import OrderedDict\n",
    "sorted_agg_dict = dict(sorted(shap_vals.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Plotting the aggregated feature importance\n",
    "\n",
    "aggregated_data_importance = pd.DataFrame.from_dict(sorted_agg_dict, orient='index',columns=['rank'])\n",
    "aggregated_data_importance.reset_index(inplace=True)\n",
    "aggregated_data_importance.rename(columns={\"index\": \"name\"}, inplace=True)\n",
    "top_10 = aggregated_data_importance.head(15)\n",
    "# print(top_10)\n",
    "\n",
    "ax = sns.barplot(x=\"rank\", y=\"name\", data=top_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinding both\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   \n",
    "    z.update(y)    \n",
    "    return z\n",
    "\n",
    "both_dict = merge_two_dicts(sorted_dict, sorted_agg_dict)\n",
    "both_dict_sorted = dict(sorted(both_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Plotting the aggregated feature importance\n",
    "\n",
    "aggregated_data_importance = pd.DataFrame.from_dict(both_dict_sorted, orient='index',columns=['rank'])\n",
    "aggregated_data_importance.reset_index(inplace=True)\n",
    "aggregated_data_importance = aggregated_data_importance.drop(aggregated_data_importance.index[0])\n",
    "aggregated_data_importance.rename(columns={\"index\": \"name\"}, inplace=True)\n",
    "top_10 = aggregated_data_importance.head(20)\n",
    "# print(top_10)\n",
    "\n",
    "ax = sns.barplot(x=\"rank\", y=\"name\", data=top_10,palette=(\"PuRd\"), alpha=0.8, linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = ax.get_figure() \n",
    "figure.set_size_inches(20, 8)\n",
    "\n",
    "#figure.tight_layout()\n",
    "figure.savefig('/home/kiwitn01/master_thesis_hypertension-complications/Deep_Learning/SHAP/Final_SHAP_DatasetA/All3_Final_Shap_sum_values_final_without_age.png', bbox_inches='tight', dbi = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#always save under correct name\n",
    "pd.DataFrame.from_dict(data=both_dict_sorted, orient='index').to_csv('/home/kiwitn01/master_thesis_hypertension-complications/Deep_Learning/SHAP/Final_SHAP_DatasetA/SHAP_lists/All3_sum_values_final.csv', header=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
